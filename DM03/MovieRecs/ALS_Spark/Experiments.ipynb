{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/emil/.conda/envs/python2/bin/python2.7\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/home/emil/.conda/envs/python2/bin/python2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "data=sc.textFile(\"train.txt\", use_unicode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: [float(item) for item in x.split(\"\\t\")])\n",
    "df = spark.createDataFrame(data, [\"uid\", \"mid\", \"rat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol=\"uid\", itemCol=\"mid\", ratingCol=\"rat\",\n",
    "          coldStartStrategy=\"drop\", nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(als.rank, [10,11,12,13])\n",
    "              .addGrid(als.maxIter, [16,17,18,19])\n",
    "              .addGrid(als.regParam, [.15,.16,.17,.18])\n",
    "              .build())\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rat', predictionCol='prediction')\n",
    "tvs = TrainValidationSplit(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.922835484741\n"
     ]
    }
   ],
   "source": [
    "best_model = model.bestModel\n",
    "predictions = best_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "19\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "print(best_model.rank)\n",
    "print(best_model._java_obj.parent().getMaxIter())\n",
    "print(best_model._java_obj.parent().getRegParam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "als1 = ALS(userCol=\"uid\", itemCol=\"mid\", ratingCol=\"rat\",\n",
    "           nonnegative=True,\n",
    "          rank=13, maxIter=19, regParam=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als1.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=sc.textFile(\"test.txt\", use_unicode=False)\n",
    "data_test = data_test.map(lambda x: [float(item) for item in x.split(\"\\t\")])\n",
    "df_test = spark.createDataFrame(data_test, [\"uid\", \"mid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9430\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_test)\n",
    "print(predictions.count())\n",
    "\n",
    "column = \"prediction\"\n",
    "predictions = predictions.withColumn(column,F.when(F.isnan(F.col(column)),1).otherwise(F.col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predictions.sort([\"uid\", \"mid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = predictions.rdd.map(tuple)\n",
    "\n",
    "with open(\"submission-3\", \"w\") as f:\n",
    "    f.write(\"Id,Score\\n\") \n",
    "    for idx, row in enumerate(rdd.collect()):\n",
    "        f.write(str(idx+1)+\",\"+str(row[2])+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
