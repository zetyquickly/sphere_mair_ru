{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LambdaMART and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA = 1.0 # from an article any sigma would work\n",
    "LIMIT_DELTA = 50 # limit for exponent\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, asessor_scores, test_mode=False):\n",
    "        self.test_mode = test_mode\n",
    "        self.count_docs = len(asessor_scores)\n",
    "        if self.test_mode:\n",
    "            self.update_scores(np.ones((self.count_docs, )))\n",
    "        else:\n",
    "            self.asessor_scores = np.copy(asessor_scores)\n",
    "            scores_sorted = np.sort(self.asessor_scores)[::-1]\n",
    "            self.dcg_norm = np.sum((2.0 ** scores_sorted - 1) / np.log(np.arange(2, self.count_docs+2)))\n",
    "            if self.dcg_norm == 0:\n",
    "                self.dcg_norm = 1.0\n",
    "\n",
    "            self.permutations = np.tile(np.arange(0, self.count_docs), (self.count_docs, 1))\n",
    "            self.update_scores(np.zeros((self.count_docs, )))\n",
    "\n",
    "    def update_delta_ndcg(self):\n",
    "        self.delta_ndcg = (-1.0 / np.log(self.positions.reshape(-1, 1)+1) + 1.0 / np.log(self.positions[self.permutations]+1))\n",
    "        self.delta_ndcg *=(((2 ** self.asessor_scores.reshape(-1, 1)) - 1) - ((2 ** self.asessor_scores[self.permutations]) - 1))\n",
    "        self.delta_ndcg = np.abs(self.delta_ndcg) / self.dcg_norm\n",
    "\n",
    "    def get_ndcg(self):\n",
    "        return np.sum((2.0 ** self.asessor_scores - 1) / np.log(self.positions+1)) / self.dcg_norm\n",
    "\n",
    "    def update_scores(self, new_scores):\n",
    "\n",
    "        # getting current of scores\n",
    "        self.positions = np.zeros((self.count_docs, ), dtype=np.int32)\n",
    "        self.positions[np.argsort(new_scores)[::-1].astype(np.int32)] = np.arange(1, self.count_docs+1)\n",
    "        self.scores = np.copy(new_scores)\n",
    "        if self.test_mode == True:\n",
    "            return\n",
    "\n",
    "        self.update_delta_ndcg()\n",
    "\n",
    "        delta_scores = np.abs(SIGMA * (self.scores.reshape((-1, 1)) - self.scores[self.permutations]))\n",
    "        delta_scores[delta_scores >= LIMIT_DELTA / SIGMA] = LIMIT_DELTA / SIGMA\n",
    "        self.ro_ij = 1.0 / (1 + np.exp(SIGMA * delta_scores))\n",
    "\n",
    "        correct_permutations = ((self.asessor_scores.reshape((-1, 1)) \n",
    "                               > self.asessor_scores[self.permutations]).astype(np.int8))\n",
    "        incorrect_permutations = ((self.asessor_scores.reshape((-1, 1)) \n",
    "                                 < self.asessor_scores[self.permutations]).astype(np.int8))\n",
    "        valid_permutations = correct_permutations + incorrect_permutations\n",
    "\n",
    "        # update gradient\n",
    "        self.numerators = -np.sum(self.delta_ndcg * self.ro_ij * correct_permutations - \\\n",
    "                                  self.delta_ndcg * self.ro_ij * incorrect_permutations, axis=1)\n",
    "\n",
    "        # update hessian\n",
    "        self.denominators = np.sum(self.delta_ndcg * SIGMA * self.ro_ij \n",
    "                                   * (1.0 - self.ro_ij) * valid_permutations, axis=1)\n",
    "        self.denominators[self.denominators == 0] = 1\n",
    "\n",
    "        \n",
    "class Data:\n",
    "    def __init__(self, test_mode=False):\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        print(\"Loading data \"+ datetime.now().isoformat())\n",
    "        self.X, self.y, documents_query = load_svmlight_file(filename, query_id=True)\n",
    "        print(\"Data has been loaded! \"+ datetime.now().isoformat())\n",
    "\n",
    "        print(\"Creating queries... \"+ datetime.now().isoformat())\n",
    "        self.queries = []\n",
    "        self.query_document_indices = []\n",
    "        self.unique_query_indices = np.unique(documents_query)\n",
    "        for query_id in self.unique_query_indices:\n",
    "            self.query_document_indices.append(np.where(documents_query == query_id)[0])\n",
    "            self.queries.append(Query(self.y[self.query_document_indices[-1]], self.test_mode))\n",
    "            if query_id % 1000 == 0:\n",
    "                print(query_id)\n",
    "        print(\"Queries have been created \"+ datetime.now().isoformat())\n",
    "        return self\n",
    "\n",
    "EPOCH = 0\n",
    "def ObjectiveFunction(data):\n",
    "    def _objective_function(y_true, y_pred):\n",
    "        global EPOCH\n",
    "        print(\"Epoch = \" + str(EPOCH) + \"; \" + datetime.now().isoformat())\n",
    "        EPOCH += 1\n",
    "        for query_id, indices in enumerate(data.query_document_indices):\n",
    "            data.queries[query_id].update_scores(y_pred[indices])\n",
    "\n",
    "        return np.hstack(q.numerators for q in train_data.queries), \\\n",
    "              np.hstack(q.denominators for q in train_data.queries)\n",
    "    return _objective_function\n",
    "\n",
    "\n",
    "class LambdaMART:\n",
    "    def __init__(self, train_data, **kwargs):\n",
    "        objective_func = ObjectiveFunction(train_data)\n",
    "        self.xgb_classifier = XGBRegressor(n_jobs=4, objective=objective_func, **kwargs)\n",
    "\n",
    "    def fit(self, train_data):\n",
    "       self.xgb_classifier.fit(train_data.X, train_data.y)\n",
    "       return self\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        results = self.xgb_classifier.predict(test_data.X)\n",
    "        for query_id, indices in enumerate(test_data.query_document_indices):\n",
    "             test_data.queries[query_id].update_scores(results[indices])\n",
    "\n",
    "def create_submission(filename, test_data):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"QueryId,DocumentId\\n\")\n",
    "        document_base_idx = 0\n",
    "        for query_idx, unique_query_idx in enumerate(test_data.unique_query_indices):\n",
    "            doc_pos = test_data.queries[query_idx].positions\n",
    "            pos_for_write = np.full((len(doc_pos), ), document_base_idx)\n",
    "            pos_for_write[doc_pos-1] += np.arange(1, len(doc_pos)+1)\n",
    "            for pos in pos_for_write:\n",
    "                f.write(str(unique_query_idx)+\",\"+str(pos)+\"\\n\")\n",
    "            document_base_idx += len(test_data.queries[query_idx].positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_xgb_0.2-10-8-hist.txt\n",
      "Loading data 2019-02-10T22:07:32.057997\n",
      "Data has been loaded! 2019-02-10T22:09:31.776998\n",
      "Creating queries... 2019-02-10T22:09:31.777150\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "Queries have been created 2019-02-10T22:09:47.640583\n",
      "[22:10:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "Epoch = 0; 2019-02-10T22:10:22.075735\n",
      "Epoch = 1; 2019-02-10T22:10:40.481070\n",
      "Epoch = 2; 2019-02-10T22:10:48.446199\n",
      "Loading data 2019-02-10T22:10:57.001198\n",
      "Data has been loaded! 2019-02-10T22:11:49.603243\n",
      "Creating queries... 2019-02-10T22:11:49.603449\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "Queries have been created 2019-02-10T22:11:51.556833\n"
     ]
    }
   ],
   "source": [
    "filename_base = \"sm_xgb_0.2-10-8-hist.txt\"\n",
    "print(filename_base)\n",
    "\n",
    "train_data = Data().load_data(\"/home/emil/Rank/train.txt\")\n",
    "params = {'max_depth': 8, 'silent': 1, 'learning_rate': 0.2, 'tree_method': 'hist', \n",
    "          'n_estimators': 3}\n",
    "model = LambdaMART(train_data, **params)\n",
    "model.fit(train_data)\n",
    "\n",
    "test_data = Data(test_mode=True).load_data(\"/home/emil/Rank/test.txt\")\n",
    "model.predict(test_data)\n",
    "create_submission(\"submission_\"+filename_base, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
